{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "d7sKSxZC0mlq"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import torchtext\n",
        "import torchtext.transforms as T\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchlake.common.schemas.nlp import NlpContext\n",
        "from torchlake.common.utils.platform import get_file_size, get_num_workers\n",
        "from torchlake.common.utils.text import (build_vocab, get_context,\n",
        "                                         get_unigram_counts, is_corpus_title,\n",
        "                                         is_longer_text)\n",
        "from torchlake.representation.models import HellingerPCA\n",
        "from torchlake.representation.models.hellinger.helper import \\\n",
        "    CoOccurrenceCounter\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.datasets import WikiText2, WikiText103"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To utilize this notebook, you have to install `portalocker` first"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Gcf1yr-Y0mls",
        "outputId": "d5f96f63-22d9-4857-d230-af686ab5df12"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'2.1.0+cu118'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Iz1byBBS0mlu",
        "outputId": "7057b39e-a226-43ce-97ee-4e54f9f5cf31"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'0.16.0+cpu'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torchtext.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQC-kmj40mlv"
      },
      "source": [
        "# setting\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "DATASET_NAME = \"WikiText2\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "artifact_path = Path('../../artifacts/hellinger-pca')\n",
        "artifact_path.mkdir(exist_ok=True)\n",
        "\n",
        "artifact_dataset_path = artifact_path / DATASET_NAME\n",
        "artifact_dataset_path.mkdir(exist_ok=True)\n",
        "\n",
        "data_path = Path('../../data') / DATASET_NAME"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "CONTEXT = NlpContext(device=\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "BATCH_SIZE = 32\n",
        "CONTEXT_SIZE = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "DEVICE = torch.device(CONTEXT.device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = get_tokenizer('basic_english')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "VjIy_PjO0ml6"
      },
      "outputs": [],
      "source": [
        "def datapipe_factory(datapipe, context_size: int = 1, transform = None):\n",
        "    datapipe = (\n",
        "        datapipe\n",
        "        .map(lambda text: text.strip())\n",
        "        .map(lambda text: text.lower())\n",
        "        .filter(lambda text: is_longer_text(text, context_size))\n",
        "        .filter(lambda text: not is_corpus_title(text))\n",
        "        .map(tokenizer)\n",
        "    )\n",
        "\n",
        "    if transform:\n",
        "      datapipe = datapipe.map(transform)\n",
        "\n",
        "    return datapipe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "IJsLMBXd0ml5"
      },
      "outputs": [],
      "source": [
        "train_datapipe, val_datapipe, test_datapipe = WikiText2(data_path.as_posix())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDQE8SdK0ml5",
        "outputId": "25f19425-0b0b-4158-f7c5-a5e4281e10a4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\research\\pytorch-implementations\\.venv\\Lib\\site-packages\\torch\\utils\\data\\datapipes\\utils\\common.py:141: UserWarning: Local function is not supported by pickle, please use regular python function or functools.partial instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "cloned_train_datapipe = datapipe_factory(train_datapipe, CONTEXT_SIZE)\n",
        "vocab = build_vocab(cloned_train_datapipe)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "20351"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "VOCAB_SIZE = len(vocab)\n",
        "VOCAB_SIZE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39QfrYufkefY",
        "outputId": "fdc52a35-2910-4d6c-fb09-a074df56d240"
      },
      "outputs": [],
      "source": [
        "# write_json_file(\n",
        "#     artifact_dataset_path.joinpath(\"vocab.json\"),\n",
        "#     list(vocab.get_stoi().keys()),\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5eL12nFLH_M",
        "outputId": "60f7e4e4-09ef-41b6-e388-7079bc5f6c46"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['<unk>',\n",
              " '<bos>',\n",
              " '<eos>',\n",
              " '<pad>',\n",
              " 'the',\n",
              " ',',\n",
              " '.',\n",
              " 'of',\n",
              " 'and',\n",
              " 'in',\n",
              " 'to',\n",
              " 'a',\n",
              " 'was',\n",
              " \"'\",\n",
              " '@-@',\n",
              " 'on',\n",
              " 'as',\n",
              " 's',\n",
              " 'that',\n",
              " 'for']"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocab.lookup_tokens(range(20))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "VXZtp3Xk0ml5"
      },
      "outputs": [],
      "source": [
        "text_transform = T.Sequential(\n",
        "    T.VocabTransform(vocab),\n",
        "    T.Truncate(CONTEXT.max_seq_len - 2),\n",
        "    T.AddToken(token=CONTEXT.bos_idx, begin=True),\n",
        "    T.AddToken(token=CONTEXT.eos_idx, begin=False),\n",
        "    T.ToTensor(),\n",
        "    T.PadTransform(CONTEXT.max_seq_len, CONTEXT.padding_idx),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "1q159o50PHMf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\research\\pytorch-implementations\\.venv\\Lib\\site-packages\\torch\\utils\\data\\datapipes\\utils\\common.py:141: UserWarning: Local function is not supported by pickle, please use regular python function or functools.partial instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "train_datapipe = datapipe_factory(\n",
        "    train_datapipe,\n",
        "    CONTEXT_SIZE,\n",
        "    text_transform,\n",
        ")\n",
        "\n",
        "val_datapipe = datapipe_factory(\n",
        "    val_datapipe,\n",
        "    CONTEXT_SIZE,\n",
        "    text_transform,\n",
        ")\n",
        "\n",
        "test_datapipe = datapipe_factory(\n",
        "    test_datapipe,\n",
        "    CONTEXT_SIZE,\n",
        "    text_transform,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "NUM_WORKERS = get_num_workers()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "DL2__1kp0ml8"
      },
      "outputs": [],
      "source": [
        "collate_fn = lambda data: get_context(\n",
        "    data,\n",
        "    CONTEXT_SIZE - 1,\n",
        "    0,\n",
        "    enable_symmetric_context=False,\n",
        "    flatten_output=True,\n",
        ")\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    train_datapipe,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    collate_fn=collate_fn,\n",
        "    pin_memory=True,\n",
        "    num_workers=NUM_WORKERS,\n",
        ")\n",
        "\n",
        "val_dataloader = DataLoader(\n",
        "    val_datapipe,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    collate_fn=collate_fn,\n",
        "    pin_memory=True,\n",
        "    num_workers=NUM_WORKERS,\n",
        ")\n",
        "\n",
        "test_dataloader = DataLoader(\n",
        "    test_datapipe,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    collate_fn=collate_fn,\n",
        "    pin_memory=True,\n",
        "    num_workers=NUM_WORKERS,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "oz2f4qOd0ml7"
      },
      "outputs": [],
      "source": [
        "# check context size\n",
        "# for data in train_datapipe:\n",
        "#     if len(data) < 5:\n",
        "#         print(data)\n",
        "#         break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sd9gWgCsLymY",
        "outputId": "6b781224-caf6-461d-fd69-83d9fe50c1dc"
      },
      "outputs": [],
      "source": [
        "# sample\n",
        "# for data in train_datapipe:\n",
        "#     if len(data) > 5:\n",
        "#         print(data)\n",
        "#         print(vocab.lookup_tokens(data))\n",
        "#         break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a42AV49hp6vX",
        "outputId": "49b37f15-c86e-450b-a4ff-2335b471fee0"
      },
      "outputs": [],
      "source": [
        "# count = 0\n",
        "# for data in train_datapipe:\n",
        "#   count += len(data)\n",
        "# count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frdhH6YwqDau",
        "outputId": "05768484-8667-4e4b-8add-6448602de244"
      },
      "outputs": [],
      "source": [
        "# number of words in training corpurs\n",
        "# wikitext2 count 1,993,228\n",
        "# wikitext103 count 101,227,641"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "c2mRhse60ml8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([8160, 1]) torch.Size([8160, 1])\n"
          ]
        }
      ],
      "source": [
        "for gram, context in train_dataloader:\n",
        "    print(gram.shape, context.shape)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Br5_RcSU0ml_"
      },
      "source": [
        "# training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\research\\pytorch-implementations\\.venv\\Lib\\site-packages\\torch\\utils\\data\\datapipes\\iter\\combining.py:333: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n",
            "  warnings.warn(\"Some child DataPipes are not exhausted when __iter__ is called. We are resetting \"\n"
          ]
        }
      ],
      "source": [
        "word_counts = get_unigram_counts(\n",
        "    map(lambda x: x.tolist(), train_datapipe),\n",
        "    VOCAB_SIZE,\n",
        ").to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(4485120)"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sum(word_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "counter = CoOccurrenceCounter(VOCAB_SIZE, CONTEXT.padding_idx)\n",
        "\n",
        "for gram, context in train_dataloader:\n",
        "    counter.update_counts(gram, context)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4 7 16928\n",
            "2 6 14193\n",
            "17 13 13596\n",
            "4 9 11584\n",
            "5 0 11460\n"
          ]
        }
      ],
      "source": [
        "for (g, c), count in counter.counts.most_common(5):\n",
        "    print(g, c, count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "# in paper, they used 10000 context words from 170000 vocabulary\n",
        "model = HellingerPCA(VOCAB_SIZE, maximum_context_size=VOCAB_SIZE // 20).to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 1: Build co-occurrence matrix\n",
            "Step 2: Select most significant context words\n",
            "Step 1: Compute the kernel matrix\n",
            "Step 2: Center the kernel matrix\n",
            "Step 3: Eigenvalue decomposition\n"
          ]
        }
      ],
      "source": [
        "model.fit(counter, word_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 1: Build co-occurrence matrix\n",
            "Step 2: Select most significant context words\n",
            "Step 1: Compute the kernel matrix\n",
            "Step 2: Center the kernel matrix\n",
            "Step 3: Eigenvalue decomposition\n"
          ]
        }
      ],
      "source": [
        "model.fit(counter, word_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([20351, 50])"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.get_embedding().shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([256, 50])"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.transform(next(iter(train_datapipe))).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Save & Load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_path = artifact_path / f\"hellinger-pca.{DATASET_NAME}.pth\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.save(model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'1.62GB'"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "get_file_size(model_path, \"G\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.load(model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7OmkYxH1jIT"
      },
      "source": [
        "# Evaluation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKBBG3gE0mmJ"
      },
      "source": [
        "## word analogy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torchlake.language_model.datasets import WordAnalogyDataset "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "analogy_data_path = Path('../../data') / \"word-analogy\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset = WordAnalogyDataset(analogy_data_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('Athens', 'Greece', 'Baghdad', 'Iraq'),\n",
              " ('Athens', 'Greece', 'Bangkok', 'Thailand'),\n",
              " ('Athens', 'Greece', 'Beijing', 'China'),\n",
              " ('Athens', 'Greece', 'Berlin', 'Germany'),\n",
              " ('Athens', 'Greece', 'Bern', 'Switzerland'),\n",
              " ('Athens', 'Greece', 'Cairo', 'Egypt'),\n",
              " ('Athens', 'Greece', 'Canberra', 'Australia'),\n",
              " ('Athens', 'Greece', 'Hanoi', 'Vietnam'),\n",
              " ('Athens', 'Greece', 'Havana', 'Cuba'),\n",
              " ('Athens', 'Greece', 'Helsinki', 'Finland'),\n",
              " ('Athens', 'Greece', 'Islamabad', 'Pakistan'),\n",
              " ('Athens', 'Greece', 'Kabul', 'Afghanistan'),\n",
              " ('Athens', 'Greece', 'London', 'England'),\n",
              " ('Athens', 'Greece', 'Madrid', 'Spain'),\n",
              " ('Athens', 'Greece', 'Moscow', 'Russia'),\n",
              " ('Athens', 'Greece', 'Oslo', 'Norway'),\n",
              " ('Athens', 'Greece', 'Ottawa', 'Canada'),\n",
              " ('Athens', 'Greece', 'Paris', 'France'),\n",
              " ('Athens', 'Greece', 'Rome', 'Italy'),\n",
              " ('Athens', 'Greece', 'Stockholm', 'Sweden')]"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset.data[:20]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[12958, 6970, 18304, 5313],\n",
              " [12958, 6970, 9016, 6480],\n",
              " [12958, 6970, 10276, 1346],\n",
              " [12958, 6970, 4686, 1270],\n",
              " [12958, 6970, 0, 4409],\n",
              " [12958, 6970, 14017, 2280],\n",
              " [12958, 6970, 16617, 481],\n",
              " [12958, 6970, 14323, 1572],\n",
              " [12958, 6970, 17076, 6546],\n",
              " [12958, 6970, 0, 6575]]"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokens = [list(vocab[word.lower()] for word in pairs) for pairs in dataset.data]\n",
        "tokens[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "Rlg9lLio0mmK"
      },
      "outputs": [],
      "source": [
        "vectors = torch.stack([model.transform(token) for token in tokens if len(token) == 4])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.25176307559013367\n"
          ]
        }
      ],
      "source": [
        "# word: 25.2\n",
        "metric = nn.CosineSimilarity()\n",
        "\n",
        "country_a, capital_a, country_b, capital_b = vectors.transpose(0, 1)\n",
        "score = metric(country_a - capital_a - country_b, -capital_b)\n",
        "print(score.mean().item())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.11.4 ('.venv': pipenv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "f35bd473d876cea6bd28a6266f5aa5f9962dc64f5f4db6f1a1cfdf1b2fe9cc07"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
