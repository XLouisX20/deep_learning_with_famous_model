{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchinfo\n",
    "import torchtext.transforms as T\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchlake.common.schemas import NlpContext\n",
    "from torchlake.common.utils.text import build_vocab\n",
    "from torchlake.sequence_data.models import (Seq2Seq, Seq2SeqDecoder, Seq2SeqAttentionEncoder,\n",
    "                                            Seq2SeqEncoder)\n",
    "from torchlake.sequence_data.models.seq2seq.network import GlobalAttention, LocalAttention\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.datasets import Multi30k\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(\"../../data/multi30k\")\n",
    "artifacts_path = Path(\"../../artifacts/seq2seq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "context = NlpContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(context.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_LANGUAGE = 'de'\n",
    "TRG_LANGUAGE = 'en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\research\\pytorch-implementations\\.venv\\Lib\\site-packages\\torchtext\\data\\utils.py:105: UserWarning: Spacy model \"de\" could not be loaded, trying \"de_core_news_sm\" instead\n",
      "  warnings.warn(\n",
      "d:\\research\\pytorch-implementations\\.venv\\Lib\\site-packages\\torchtext\\data\\utils.py:105: UserWarning: Spacy model \"en\" could not be loaded, trying \"en_core_web_sm\" instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizers = {\n",
    "    SRC_LANGUAGE: get_tokenizer('spacy', language=SRC_LANGUAGE), # de_core_news_sm\n",
    "    TRG_LANGUAGE: get_tokenizer('spacy', language=TRG_LANGUAGE)  # en_web_core_sm\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, val_iter, test_iter = Multi30k(\n",
    "    data_path.as_posix(),\n",
    "    language_pair=(SRC_LANGUAGE, TRG_LANGUAGE),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\research\\pytorch-implementations\\.venv\\Lib\\site-packages\\torch\\utils\\data\\datapipes\\iter\\combining.py:333: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n",
      "  warnings.warn(\"Some child DataPipes are not exhausted when __iter__ is called. We are resetting \"\n"
     ]
    }
   ],
   "source": [
    "vocabs = {\n",
    "    SRC_LANGUAGE: build_vocab(map(lambda x: tokenizers[SRC_LANGUAGE](x[0]), train_iter), context),\n",
    "    TRG_LANGUAGE: build_vocab(map(lambda x: tokenizers[SRC_LANGUAGE](x[1]), train_iter), context),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_transform = T.Sequential(\n",
    "    T.VocabTransform(vocabs[SRC_LANGUAGE]),\n",
    "    T.Truncate(context.max_seq_len - 2),\n",
    "    T.AddToken(token=context.bos_idx, begin=True),\n",
    "    T.AddToken(token=context.eos_idx, begin=False),\n",
    "    T.ToTensor(),\n",
    "    T.PadTransform(context.max_seq_len, context.padding_idx),\n",
    ")\n",
    "\n",
    "trg_transform = T.Sequential(\n",
    "    T.VocabTransform(vocabs[TRG_LANGUAGE]),\n",
    "    T.Truncate(context.max_seq_len - 2),\n",
    "    T.AddToken(token=context.bos_idx, begin=True),\n",
    "    T.AddToken(token=context.eos_idx, begin=False),\n",
    "    T.ToTensor(),\n",
    "    T.PadTransform(context.max_seq_len, context.padding_idx),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    src_batch, tgt_batch = [], []\n",
    "\n",
    "    for src_sample, trg_sample in batch:\n",
    "        # paper p.2: reverse source sentence\n",
    "        src_sample = src_sample[::-1]\n",
    "        \n",
    "        src_tokenizer  = tokenizers[SRC_LANGUAGE]\n",
    "        src_batch.append(src_transform(src_tokenizer(src_sample.rstrip(\"\\n\"))))\n",
    "        \n",
    "        trg_tokenizer  = tokenizers[TRG_LANGUAGE]\n",
    "        tgt_batch.append(trg_transform(trg_tokenizer(trg_sample.rstrip(\"\\n\"))))\n",
    "\n",
    "    return torch.stack(src_batch), torch.stack(tgt_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_iter,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    collate_fn=collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 256]) torch.Size([32, 256])\n"
     ]
    }
   ],
   "source": [
    "for src, trg in train_loader:\n",
    "    print(src.shape, trg.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_sizes = {\n",
    "    SRC_LANGUAGE: len(vocabs[SRC_LANGUAGE]),\n",
    "    TRG_LANGUAGE: len(vocabs[TRG_LANGUAGE]),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "\n",
    "encoder = Seq2SeqEncoder(\n",
    "    vocab_sizes[SRC_LANGUAGE],\n",
    "    128,\n",
    "    128,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    bidirectional=BIDIRECTIONAL,\n",
    "    context=context,\n",
    ")\n",
    "\n",
    "# encoder = Seq2SeqAttentionEncoder(\n",
    "#     vocab_sizes[SRC_LANGUAGE],\n",
    "#     128,\n",
    "#     128,\n",
    "#     num_layers=NUM_LAYERS,\n",
    "#     bidirectional=BIDIRECTIONAL,\n",
    "#     context=context,\n",
    "# )\n",
    "\n",
    "decoder = Seq2SeqDecoder(\n",
    "    vocab_sizes[TRG_LANGUAGE],\n",
    "    128,\n",
    "    128,\n",
    "    output_size=vocab_sizes[TRG_LANGUAGE],\n",
    "    num_layers=NUM_LAYERS,\n",
    "    bidirectional=BIDIRECTIONAL,\n",
    "    context=context,\n",
    ")\n",
    "\n",
    "attention = None \n",
    "# attention = GlobalAttention(128, num_layers=NUM_LAYERS, bidirectional=BIDIRECTIONAL)\n",
    "# attention = LocalAttention(128, num_layers=NUM_LAYERS, bidirectional=BIDIRECTIONAL)\n",
    "\n",
    "model = Seq2Seq(encoder, decoder, attention, context=context).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "Seq2Seq                                  --\n",
       "├─Seq2SeqEncoder: 1-1                    --\n",
       "│    └─LstmClassifier: 2-1               --\n",
       "│    │    └─Embedding: 3-1               464,640\n",
       "│    │    └─LSTM: 3-2                    659,456\n",
       "│    │    └─LayerNorm: 3-3               512\n",
       "├─Seq2SeqDecoder: 1-2                    --\n",
       "│    └─LstmClassifier: 2-2               --\n",
       "│    │    └─Embedding: 3-4               430,720\n",
       "│    │    └─LSTM: 3-5                    659,456\n",
       "│    │    └─LayerNorm: 3-6               512\n",
       "│    │    └─Linear: 3-7                  864,805\n",
       "=================================================================\n",
       "Total params: 3,080,101\n",
       "Trainable params: 3,080,101\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchinfo.summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index = context.padding_idx)\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoches = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]d:\\research\\pytorch-implementations\\.venv\\Lib\\site-packages\\torch\\utils\\data\\datapipes\\iter\\combining.py:333: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n",
      "  warnings.warn(\"Some child DataPipes are not exhausted when __iter__ is called. We are resetting \"\n",
      "32it [00:19,  1.80it/s]"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "training_loss = []\n",
    "for e in range(epoches):\n",
    "    running_loss = 0.0\n",
    "    data_count = 0\n",
    "    \n",
    "    for source_sentence, target_sentence in tqdm(train_loader):\n",
    "        data_count += len(source_sentence)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        text = source_sentence.to(device)\n",
    "        label = target_sentence.to(device)\n",
    "\n",
    "        output = model(text, label)\n",
    "        loss = criterion(output.transpose(-1, -2), label)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    mean_loss = running_loss / data_count\n",
    "    training_loss.append(mean_loss)\n",
    "    print(f\"epoch {e+1} : {mean_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(training_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(val_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import BLEUScore, Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\research\\pytorch-implementations\\.venv\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:62: FutureWarning: Importing `Perplexity` from `torchmetrics` was deprecated and will be removed in 2.0. Import `Perplexity` from `torchmetrics.text` instead.\n",
      "  _future_warning(\n"
     ]
    }
   ],
   "source": [
    "metric = Perplexity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]d:\\research\\pytorch-implementations\\.venv\\Lib\\site-packages\\torch\\utils\\data\\datapipes\\iter\\combining.py:333: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n",
      "  warnings.warn(\"Some child DataPipes are not exhausted when __iter__ is called. We are resetting \"\n",
      "7it [00:09,  1.34s/it]d:\\research\\pytorch-implementations\\.venv\\Lib\\site-packages\\torch\\_jit_internal.py:1355: UserWarning: The inner type of a container is lost when calling torch.jit.isinstance in eager mode. For example, List[int] would become list and therefore falsely return True for List[float] or List[str].\n",
      "  warnings.warn(\n",
      "8it [00:10,  1.30s/it]\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "running_loss = 0.0\n",
    "data_count = 0\n",
    "with torch.no_grad():\n",
    "    for source_sentence, target_sentence in tqdm(test_loader):\n",
    "        data_count += len(source_sentence)\n",
    "        text = source_sentence.to(device)\n",
    "        label = target_sentence.to(device)\n",
    "        # label = label[1:].view(-1)\n",
    "\n",
    "        output = model(text, label, 0)\n",
    "        # output = torch.flatten(output[:, 1:], end_dim=-2)\n",
    "\n",
    "        # loss = criterion(output.transpose(-1, -2), label)\n",
    "        # running_loss += loss.item()\n",
    "        metric.update(output.detach().cpu(), label.detach().cpu())\n",
    "\n",
    "    # mean_loss = running_loss / data_count\n",
    "\n",
    "# print(f\"loss: {mean_loss}\")\n",
    "# print(f\"perplexity: {math.exp(mean_loss)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(23113840.)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1句\n",
      "source: Eine Gruppe von Männern lädt Baumwolle auf einen Lastwagen\n",
      "target: A group of men are loading cotton onto a truck\n",
      "output: A group of people are loading a a a truck\n",
      "第2句\n",
      "source: Ein Mann schläft in einem grünen Raum auf einem Sofa.\n",
      "target: A man sleeping in a green room on a couch.\n",
      "output: A man sleeping in a green room with a <unk> .\n",
      "第3句\n",
      "source: Ein Junge mit Kopfhörern sitzt auf den Schultern einer Frau.\n",
      "target: A boy wearing headphones sits on a woman's shoulders.\n",
      "output: A man is headphones sits on a woman in in .\n",
      "第4句\n",
      "source: Zwei Männer bauen eine blaue Eisfischerhütte auf einem zugefrorenen See auf\n",
      "target: Two men setting up a blue ice fishing hut on an iced over lake\n",
      "output: Two men setting up a <unk> ice cream and on an <unk> over a . . .\n",
      "第5句\n",
      "source: Ein Mann mit beginnender Glatze, der eine rote Rettungsweste trägt, sitzt in einem kleinen Boot.\n",
      "target: A balding man wearing a red life jacket is sitting in a small boat.\n",
      "output: A man in wearing a blue life jacket is holding holding a <unk> boat and a a daily . . . accordion . .\n",
      "第6句\n",
      "source: Eine Frau in einem rotem Mantel, die eine vermutlich aus Asien stammende Handtasche in einem blauen Farbton hält, springt für einen Schnappschuss in die Luft.\n",
      "target: A lady in a red coat, holding a bluish hand bag likely of asian descent, jumping off the ground for a snapshot.\n",
      "output: A man in in in coat , and a a a a a of a descent , and off a ground and a <unk> .\n",
      "第7句\n",
      "source: Ein brauner Hund rennt dem schwarzen Hund hinterher.\n",
      "target: A brown dog is running after the black dog.\n",
      "output: A brown dog is playing a the black and .\n",
      "第8句\n",
      "source: Ein kleiner Junge mit einem Giants-Trikot schwingt einen Baseballschläger in Richtung eines ankommenden Balls.\n",
      "target: A young boy wearing a Giants jersey swings a baseball bat at an incoming pitch.\n",
      "output: A young girl in a blue shirt swings on baseball bat at an <unk> .\n",
      "第9句\n",
      "source: Ein Mann telefoniert in einem unaufgeräumten Büro\n",
      "target: A man in a cluttered office is using the telephone\n",
      "output: A man in a cluttered office is <unk> <unk> telephone\n",
      "第10句\n",
      "source: Eine lächelnde Frau mit einem pfirsichfarbenen Trägershirt hält ein Mountainbike\n",
      "target: A smiling woman in a peach tank top stands holding a mountain bike\n",
      "output: A man is in in <unk> tank top and a a . .\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = 100\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, (source_sentence, target_sentence) in enumerate(val_iter):\n",
    "        ori_source, ori_target = source_sentence, target_sentence\n",
    "        source_sentence = tokenizers[SRC_LANGUAGE](source_sentence.rstrip('\\n'))\n",
    "        source_sentence = src_transform(source_sentence)\n",
    "        source_sentence = source_sentence.unsqueeze(0)\n",
    "        source_sentence = source_sentence.to(device)\n",
    "        \n",
    "        target_sentence = tokenizers[TRG_LANGUAGE](target_sentence.rstrip('\\n'))\n",
    "        target_sentence = trg_transform(target_sentence)\n",
    "        target_sentence = target_sentence.unsqueeze(0)\n",
    "        target_sentence = target_sentence.to(device)\n",
    "        \n",
    "        # outputs = [BOS_IDX]\n",
    "        \n",
    "        # h, c = model.encoder(source_sentence)\n",
    "        \n",
    "        # for _ in range(MAX_LENGTH):\n",
    "            \n",
    "        #     input_seq = torch.LongTensor([outputs[-1]]).to(device)\n",
    "            \n",
    "        #     output, _, _ = model.decoder(input_seq, h, c)\n",
    "            \n",
    "        #     next_token = output.argmax(1).item()\n",
    "        #     outputs.append(next_token) \n",
    "            \n",
    "        #     if next_token == EOS_IDX: break\n",
    "        \n",
    "        output = model(source_sentence, target_sentence)[0].argmax(-1)\n",
    "        \n",
    "        # if EOS_IDX in outputs:\n",
    "        translated = vocabs[TRG_LANGUAGE].lookup_tokens(output.tolist())\n",
    "        start_idx, end_idx = translated.index(context.bos_str), translated.index(context.eos_str)\n",
    "        print(f'第{i+1}句')\n",
    "        print('source:', ori_source.rstrip('\\n'))\n",
    "        print('target:', ori_target.rstrip('\\n'))\n",
    "        print('output:', *translated[start_idx+1:end_idx], sep=' ')\n",
    "        \n",
    "        if i+1==10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = artifacts_path / 'seq2seq.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, model_path.as_posix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(model_path.as_posix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
