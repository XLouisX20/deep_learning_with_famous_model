# PyTorch Implementations

[english-version](https://github.com/gitE0Z9/pytorch-implementations/blob/main/README.en.md)

[中文版本](https://github.com/gitE0Z9/pytorch-implementations/blob/main/README.md)

The articles are simultaneously published on my personal blog (https://gite0z9.github.io).

The original intention of this project is to implement deep learning models I have learned over the years, providing notebooks and Medium articles for learning. This project is not a production-ready library for a specific domain but serves as a resource for learning.

The development direction will extend towards a monorepo.

<table>
  <tr>
    <th>Domain</th>
    <th>Model</th>
  </tr>
  <tr>
    <td>
      <b>Few shot learning</b>
    </td>
    <td>
      <ul>
        <li>
          <a href="https://acrocanthosaurus627.medium.com/%E7%B6%93%E5%85%B8%E7%B6%B2%E8%B7%AF%E7%B3%BB%E5%88%97-%E4%B8%80-siamese-network-c06dc78242ed">
            Siamese network
          </a>
        </li>
        <li>
          <a href="https://acrocanthosaurus627.medium.com/%E7%B6%93%E5%85%B8%E7%B6%B2%E8%B7%AF%E7%B3%BB%E5%88%97-13-prototypical-network-360f0e411d21">
            Prototypical network
          </a>
        </li>
      </ul>
    </td>
  </tr>
  <tr>
    <td>
      <b>Image classification</b>
    </td>
    <td>
      <ul>
        <li>
          <a href="https://acrocanthosaurus627.medium.com/%E7%B6%93%E5%85%B8%E7%B6%B2%E8%B7%AF%E7%B3%BB%E5%88%97-%E4%B8%83-resnet-690868d7af43">
            ResNet50
          </a>
        </li>
        <li>
          <a target="_blank" rel="noopener noreferrer" href="https://acrocanthosaurus627.medium.com/pytorch%E5%AF%A6%E4%BD%9C%E7%B3%BB%E5%88%97-resnext-019a1528cfd7">
            ResNeXt50-32x4d
          </a>
        </li>
        <li>
          <a target="_blank" rel="noopener noreferrer" href="https://acrocanthosaurus627.medium.com/pytorch%E5%AF%A6%E4%BD%9C%E7%B3%BB%E5%88%97-mobilenet-v1-v2-9224c02ff45e">
            MobileNet v1
          </a>
        </li>
        <li>
          <a target="_blank" rel="noopener noreferrer" href="https://acrocanthosaurus627.medium.com/pytorch%E5%AF%A6%E4%BD%9C%E7%B3%BB%E5%88%97-mobilenet-v1-v2-9224c02ff45e">
            MobileNet v2
          </a>
        </li>
      </ul>
    </td>
  </tr>
  <tr>
    <td>
      <b>Object detection</b>
    </td>
    <td>
      <ul>
        <li>
          <a href="https://acrocanthosaurus627.medium.com/object-detection-from-scratch-with-pytorch-yolov1-a56b49024c22">
            YOLOv1
          </a>
        </li>
        <li>
          <a href="https://acrocanthosaurus627.medium.com/object-detection-from-scratch-with-pytorch-yolov2-722c4d66cd43">
            YOLOv2
          </a>
        </li>
      </ul>
    </td>
  </tr>
  <tr>
    <td>
      <b>Semantic segmentation</b>
    </td>
    <td>
      <ul>
        <li>
          <a href="https://acrocanthosaurus627.medium.com/%E7%B6%93%E5%85%B8%E7%B6%B2%E8%B7%AF%E7%B3%BB%E5%88%97-%E5%8D%81-unet-545efa00ad99">
            UNet
          </a>
        </li>
        <li>
          <a href="https://acrocanthosaurus627.medium.com/pytorch%E5%AF%A6%E4%BD%9C%E7%B3%BB%E5%88%97-fcn-89cac059179b">
            FCN
          </a>
        </li>
        <li>
          <a href="https://acrocanthosaurus627.medium.com/pytorch%E5%AF%A6%E4%BD%9C%E7%B3%BB%E5%88%97-pspnet-8059dc329221">
            PSPNet
          </a>
        </li>
      </ul>
    </td>
  </tr>
  <tr>
    <td>
      <b>Image generation</b>
    </td>
    <td>
      <ul>
        <li>
          <a href="https://acrocanthosaurus627.medium.com/%E7%B6%93%E5%85%B8%E7%B6%B2%E8%B7%AF%E7%B3%BB%E5%88%97-%E4%BA%8C-variational-autoencoder-954596aae539">
            VAE
          </a>
        </li>
        <li>
          <a href="https://acrocanthosaurus627.medium.com/%E7%B6%93%E5%85%B8%E7%B6%B2%E8%B7%AF%E7%B3%BB%E5%88%97-%E4%B8%89-generative-adversarial-network-445ffdc297fd">
            GAN
          </a>
        </li>
        <li>
          <a href="https://acrocanthosaurus627.medium.com/%E7%B6%93%E5%85%B8%E7%B6%B2%E8%B7%AF%E7%B3%BB%E5%88%97-11-dcgan-40a78e279030">
            DCGAN
          </a>
        </li>
      </ul>
    </td>
  </tr>
  <tr>
    <td>
      <b>Optical character recognition</b>
    </td>
    <td>
      <ul>
        <li>
          <a href="https://acrocanthosaurus627.medium.com/pytorch-%E5%AF%A6%E4%BD%9C%E7%B3%BB%E5%88%97-crnn-b2a7a8fa1698">
            CRNN
          </a>
        </li>
      </ul>
    </td>
  </tr>
  <tr>
    <td>
      <b>Style transfer</b>
    </td>
    <td>
      <ul>
        <li>
          <a href="https://acrocanthosaurus627.medium.com/%E7%B6%93%E5%85%B8%E7%B6%B2%E8%B7%AF%E7%B3%BB%E5%88%97-%E4%B9%9D-image-style-transfer-371e161c5620">
            Neural style transfer
          </a>
        </li>
        <li>
          <a href="https://acrocanthosaurus627.medium.com/pytorch%E5%AF%A6%E4%BD%9C%E7%B3%BB%E5%88%97-neural-doodle-80bb55108836">
            Neural Doodle
          </a>
        </li>
        <li>
          <a href="https://acrocanthosaurus627.medium.com/pytorch-%E5%AF%A6%E4%BD%9C%E7%B3%BB%E5%88%97-fast-style-transfer-6630af677395">
            Fast style transfer
          </a>
        </li>
        <li>
          <a href="https://acrocanthosaurus627.medium.com/pytorch-%E5%AF%A6%E4%BD%9C%E7%B3%BB%E5%88%97-adain-f18fd4bca76b">
            AdaIN
          </a>
        </li>
        <li>
          <a href="https://acrocanthosaurus627.medium.com/%E7%B6%93%E5%85%B8%E7%B6%B2%E8%B7%AF%E7%B3%BB%E5%88%97-14-pix2pix-5b550c1fbb39">
            Pix2pix
          </a>
        </li>
      </ul>
    </td>
  </tr>
  <tr>
    <td>
      <b>Text classification</b>
    </td>
    <td>
      <ul>
        <li>
          <a href="https://acrocanthosaurus627.medium.com/%E7%B6%93%E5%85%B8%E7%B6%B2%E8%B7%AF%E7%B3%BB%E5%88%97-%E5%9B%9B-long-short-term-memory-21c097616641">
            LSTM
          </a>
        </li>
        <li>
          <a href="https://acrocanthosaurus627.medium.com/%E7%B6%93%E5%85%B8%E7%B6%B2%E8%B7%AF%E7%B3%BB%E5%88%97-%E4%BA%94-textcnn-cd9442139f8c">
            TextCNN
          </a>
        </li>
      </ul>
    </td>
  </tr>
  <tr>
    <td>
      <b>POS tagging</b>
    </td>
    <td>
      <ul>
        <li>
          <a href="https://acrocanthosaurus627.medium.com/pytorch-%E5%AF%A6%E4%BD%9C%E7%B3%BB%E5%88%97-bilstm-92d8e01d488e">
            BiLSTM
          </a>
        </li>
      </ul>
    </td>
  </tr>
  <tr>
    <td>
      <b>Named entity recognition</b>
    </td>
    <td>
      <ul>
        <li>
          <a href="https://acrocanthosaurus627.medium.com/pytorch-%E5%AF%A6%E4%BD%9C%E7%B3%BB%E5%88%97-bilstm-crf-7d2014a286f6">
            BiLSTM-CRF
          </a>
        </li>
      </ul>
    </td>
  </tr>
  <tr>
    <td>
      <b>Machine translation</b>
    </td>
    <td>
      <ul>
        <li>
          <a href="https://acrocanthosaurus627.medium.com/%E7%B6%93%E5%85%B8%E7%B6%B2%E8%B7%AF%E7%B3%BB%E5%88%97-%E5%85%AD-sequence-to-sequence-327886dafa4">
            Seq2Seq
          </a>
        </li>
      </ul>
    </td>
  </tr>
  <tr>
    <td>
      <b>Reinforcement learning</b>
    </td>
    <td>
      <ul>
        <li>
          <a href="https://acrocanthosaurus627.medium.com/%E7%B6%93%E5%85%B8%E7%B6%B2%E8%B7%AF%E7%B3%BB%E5%88%97-%E5%85%AB-deep-q-network-b12d7769e337">
            DQN
          </a>
        </li>
      </ul>
    </td>
  </tr>
  <tr>
    <td>
      <b>Sequence data</b>
    </td>
    <td>
      <ul>
        <li>
          <a href="https://acrocanthosaurus627.medium.com/%E7%B6%93%E5%85%B8%E7%B6%B2%E8%B7%AF%E7%B3%BB%E5%88%97-%E5%9B%9B-long-short-term-memory-21c097616641">
            LSTM
          </a>
        </li>
        <li>
          <p>GRU</p>
        </li>
        <li>
          <a href="https://acrocanthosaurus627.medium.com/%E7%B6%93%E5%85%B8%E7%B6%B2%E8%B7%AF%E7%B3%BB%E5%88%97-12-temporal-convolutional-network-799a243ffa2d">
            TCN
          </a>
        </li>
      </ul>
    </td>
  </tr>
  <tr>
    <td>
      <b>Language model</b>
    </td>
    <td>
      <ul>
        <li>
          <a href="https://acrocanthosaurus627.medium.com/language-model-from-scratch-with-pytorch-word2vec-10e77770cc57">
            Word2vec
          </a>
        </li>
      </ul>
    </td>
  </tr>
  <tr>
    <td>
      <b>Graph neural network</b>
    </td>
    <td>
      <ul>
        <li>
          <a target="_blank" rel="noopener noreferrer" href="https://acrocanthosaurus627.medium.com/pytorch-%E5%AF%A6%E4%BD%9C%E7%B3%BB%E5%88%97-gcn-c617638a9fcf">
            GCN
          </a>
        </li>
        <li>
          <a target="_blank" rel="noopener noreferrer" href="https://acrocanthosaurus627.medium.com/pytorch-%E5%AF%A6%E4%BD%9C%E7%B3%BB%E5%88%97-gat-a0a413e3cd12">
            GAT
          </a>
        </li>
      </ul>
    </td>
  </tr>
</table>

## Installation

`pip install .`

## Project structure

`notebooks`: demonstrate how to use torchlake.

`torchlake`: deep learning models composed of different domains.

In general, each domain will have a structure similar to the following:

```
├───adapter
├───artifacts
│   └───model_name
├───configs
│   └───model_name
├───constants
├───controller
├───datasets
│   └───dataset_name
├───models
│   ├───base
│   └───model_name
│       ├───model.py
│       ├───network.py
│       ├───loss.py
│       └───decode.py
├───reference
│   └───model_name
│       └───paper
├───runs
├───scripts
│   └───debug
├───tests
├───utils
└───requirements.txt
```

`adapter`: Interface between the controller and other resources (model, model.loss, etc.).

`configs`: Model configuration files, including device definition, model definition, training definition, and inference definition, covering four aspects.

`constants`: Fixed values, including constants and enums.

`controller`: Controller, where controller is a shared base, and trainer, evaluator, predictor are responsible for training, evaluation, and prediction tasks, respectively.

`models`: Model definitions, where network.py represents the model blocks, model.py is the assembled model, and loss.py is the loss function.

`datasets`: Datasets, currently categorized by domain, with raw datasets and CSV datasets. The former reads raw data, while the latter reads processed CSV data (such as normalized coordinates).

`runs`: Folder documenting TensorBoard logs for trainer and evaluator results.

`tests`: Unit tests using pytest.

`utils`: Stores functions with low dependency and high reusability.

Model and dataset configurations are controlled using pydantic.
